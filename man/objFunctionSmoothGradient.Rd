% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/smoothedLasso.r
\name{objFunctionSmoothGradient}
\alias{objFunctionSmoothGradient}
\title{Auxiliary function which computes the gradient of the smoothed L1 penalized regression operator.}
\usage{
objFunctionSmoothGradient(betavector, w, du, dv, dw, mu, entropy = TRUE)
}
\arguments{
\item{betavector}{The vector of regression coefficients.}

\item{w}{The function encoding the dependence structure among the regression coefficients.}

\item{du}{The derivative (gradient) of the objective of the regression operator.}

\item{dv}{The derivative (gradient) of the penalty of the regression operator.}

\item{dw}{The derivative (Jacobian matrix) of the function encoding the dependence structure among the regression coefficients.}

\item{mu}{The Nesterov smoothing parameter.}

\item{entropy}{A boolean switch to select the entropy prox function (default) or the squared error prox function.}
}
\value{
The value of the gradient for the input \eqn{betavector}.
}
\description{
Auxiliary function which computes the gradient of the smoothed L1 penalized regression operator.
}
\examples{
library(smoothedLasso)
n <- 100
p <- 500
betavector <- runif(p)
X <- matrix(runif(n*p),nrow=n,ncol=p)
y <- X \%*\% betavector
lambda <- 1
temp <- standardLasso(X,y,lambda)
print(objFunctionSmoothGradient(betavector,temp$w,temp$du,temp$dv,temp$dw,mu=0.1))

}
\references{
Hahn, G., Lutz, S., Laha, N., and Lange, C. (2020). A framework to efficiently smooth L1 penalties for linear regression. bioRxiv:2020.09.17.301788.
}
